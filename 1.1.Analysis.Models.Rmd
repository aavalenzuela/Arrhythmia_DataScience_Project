## Evaluation metrics


As we mention previously, we are going to implement models for two purposes:

a. **Detection of cardiac arrhythmia** 
b. **Classification of cardiac arrhythmia**

In our **first** prediction for detection of cardiac arrhythmia, the data points are classified into two classes: “Normal” & “Arrhythmia”. This model only identifies if the patient is normal (class 1) or suffers from any form of arrhythmia (class 2 to 16). Then the output is a binary variable (is a variable that has two possible outcomes). This will possible if all the instances belonging to classes 2 to 16 were merged to one class. The arrhythmia class will be treated as the ‘Positive’ class.

We are going to see the following evaluation metrics:

* **accuracy** is defined as the overall proportion that is predicted correctly.
* **sensitivity** is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive.
* **specificity** is defined as the ability of an algorithm to not predict a positive when the actual outcome is not a positive.


We can summarize in the following way:  
**High accuracy**: $Y=1 \Rightarrow  \hat{Y}=1$ and $Y=0 \Rightarrow \hat{Y}=0$  
**High sensitivity**: $Y=1 \Rightarrow \hat{Y}=1$  
**High specificity**: $Y=0 \Rightarrow \hat{Y}=0$  


We are looking for accuracy and sensitivity. Why? it is much more important to maximize sensitivity over specificity: failing to predict a arrhythmia  can put the health/life of the patience in risk. It is better than this patience classified as "Arrhythmia" and make all the exams/procedure and when validate that he or she is ok.


In our **second** prediction model for classification of cardiac arrhythmia classified the patient into one of 16 classes, with class 1 representing “Normal” and classes 2 to 16 representing a condition of cardiac arrhythmia. Here we continue with the accuracy concept but now sensitivity and specificity is for class.


Let's show the standard definition in an binary variable:  
**True Positive (TN)** – This is correctly classified as the class if interest/target.  
**True Negative (TN)** – This is correctly classified as not a class of interest/target.  
**False Positive (FP)** – This is wrongly classified as the class of interest/target.  
**False Negative (FN)** – This is wrongly classified as not a class of interest/target.  




```{r Evaluation metrics Binary Table, echo=FALSE, warning=FALSE}
M <- matrix(c("True Positives (TP)","False Positives (FP)","False Negatives (FN)","True Negatives (TN)"),2,byrow = TRUE) #%>%
rownames(M) <- c("Predicted Positive","Predicted Negative")
colnames(M) <- c("Actually Positive", "Actually Negative")

M %>% knitr::kable() %>%
  kable_styling(full_width = F)
```
An these are the definitions:

$$
Accuracy = (TP+TN)/(TP+TN+FP+FN)
$$
$$
Sensitivity = TP/(TP+FN)
$$
$$
Specificity = TN/(TN+FP)
$$


Whats happened when we have more class? Then we have the same arithmetic for the Accuracy, but we have a value for Sensitivity and Specificity for each class.


```{r Evaluation metrics Multiple Table, echo=FALSE, warning=FALSE}
M3 <- matrix(c("True A (TA)","False A (FA.B)","False A (FA.C)","False B (FB.A)","True B (TB)","False B (FB.C)","False C (FC.A)","False C (FC.B)","True C (TC)"),3,byrow = TRUE) #%>%
rownames(M3) <- c("Predicted A", "Predicted B", "Predicted C")
colnames(M3) <- c("Actually A", "Actually B", "Actually C")
M3 %>% knitr::kable() %>%
  kable_styling(full_width = F)
```

An these are the definitions:

$$
Accuracy = (TA+TB+TC)/(TA+TB+TC+FA.B+FA.C+FB.A+FB.C+FC.A+FC.B)
$$
$$
Sensitivity_{A} = TA/(TA+FB.A+FC.A)
$$
$$
Specificity_{A} = TA/(TA+FA.B+FA.C)
$$
$$
Sensitivity_{B} = TB/(TB+FA.B+FC.B)
$$
$$
Specificity_{B} = TB/(TB+FB.A+FB.C)
$$
$$
Sensitivity_{C} = TC/(TC+FB.C+FA.C)
$$
$$
Specificity_{C} = TC/(TC+FA.C+FB.C)
$$




\newpage



## First Model...just the most common

We are now to create our models. For that we are not to use the validation, we are going to use only train dataset.


The most basic and quick approach it is consider that the most common class (mode), in this case it is the "Normal" class, is the best guess. Then the model is:


$$
 \hat{y} = y_{mode} + \varepsilon_{i,u}
$$
$y_{mode}$ the "true" class for all patience and  $\varepsilon_{i,u}$ an independent errors sampled from the same distribution centered at 0.





In the code, for the prediction for the two purposes, we are going to use different dataset to make easier to read the code: the first one, that concern about “Normal” & “Arrhythmia” only, the "Code class" can only has these two values.
In the case of Classification of cardiac arrhythmia, the "Code class" has all the values allowed (13).
Because it is not a big dataset, this duplicity does not affect the computer resources available fo this project.


### First Prediction
Let's works with the first Prediction
```{r Define new dataset for first Prediction, include=FALSE}

# The first "Class code" is "Normal" and all the rest are some kind of "Arrhythmia"
# Train dataset prediction1
train_set_p1 <- train_set
train_set_p1$`Class code` <- ifelse(train_set_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# validation for prediction1
validation_set_p1 <- validation_set
validation_set_p1$`Class code` <- ifelse(validation_set_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# Expected value for validation for prediction1
#expected_value_val_p1 <- ifelse(validation_set$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

```


```{r Model just the most common first Prediction, echo=FALSE, warning=FALSE}
predicted_value <- rep("Normal", times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(predicted_value, validation_set_p1$`Class code`)  # or positive = "Arrhythmia"

cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]
```


Because an algorithm that calls everything positive ($Y = 1$ no matter what) has perfect specificity, but worse sensitivity. These are completely opposite if we set as less common.
The Accuracy obtained will be our base one.



```{r Model just the most common first Prediction Table, echo=FALSE, size='scriptsize'}

# Table names
Predicted <- predicted_value
Actually <- validation_set_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually)
```

For our record, we keep this first model:
```{r Model just the most common first Prediction Result,echo=FALSE, warning=FALSE}
Pred1_results <- data.frame(Model = "Just the most common",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"] , row.names = NULL)
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary") %>%
  kable_styling(full_width = F)
```



### Second Predictions
Second Predictions, that considered all the classes, the accuracy is the same, but now we have sensitivity and specificity by class:
```{r Model just the most common Second Prediction, echo=FALSE, warning=FALSE}
y_hat <- rep(1, times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(y_hat, validation_set$`Class code`)


cm$overall["Accuracy"]
cm$byClass[,1:2]
```


```{r Model just the most common Second Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat
Actually <- validation_set$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2)
```


We can see that only class 1 has good sensitivity, as we expected.


For our record, we keep this second model:
```{r Model just the most common Second Prediction Result, echo=FALSE, warning=FALSE}
Pred2_results <- data.frame(Model = "Just the most common",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL)
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


Then this first approach give us a base line to the next ones.




## K-Nearest Neighbors Model

Our first algorithm will be the K-Nearest Neighbors. KNN is a non-parametric method and makes no assumptions.

In base of the concept of distance, this method attempts to find K nearest points of the train data point to the test data point and assigns the class to it on basis of majority for K nearest points. Then the basic question is which value of k give a better solution.

Because we have to few data to train, we use cross-validation in our training process.

### First Prediction

Let's works with the first Prediction. In the first graph we are looking for the better k (number of Neighbors) that increase our accuracy.  This parameter will be used later, for our prediction.

```{r K-Nearest Neighbors Model first Prediction train, echo=FALSE, warning=FALSE, fig.height = 4}
knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)   


set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 20, 2)),
             data = train_set_p1, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")
```


```{r K-Nearest Neighbors Model first Prediction train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_knn
```

We get k = `r fit_knn$bestTune$k`. Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model first Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
varImp(fit_knn)
varimp_knn_p1 <- varImp(fit_knn)$importance
ggplot(varImp(fit_knn), top = 20) + ggtitle("K-Nearest Neighbors Model Top 20 feature")
```





With this model created in base of the train dataset we now take the validation dataset to see how well this apply:

```{r K-Nearest Neighbors Model first Prediction Prediction, echo=FALSE, warning=FALSE, size='scriptsize'}
y_hat_knn <- predict(fit_knn, validation_set_p1, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set_p1$`Class code`) #$overall["Accuracy"]
cm
```

We see a good improvement in accuracy and sensitivity over the previous approach. Then we see that KNN is able to create a better prediction than just the mode of the patients. We just started with this process, let's see if we can make it better,

```{r K-Nearest Neighbors Model first Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we add this model:
```{r K-Nearest Neighbors Model first Prediction Result, echo=FALSE, warning=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "K-Nearest Neighbors",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary") %>%
  kable_styling(full_width = F)

```





With this first model we have a good improvement. KNN in overall, has a poor sensitivity and very good specificity. If "Arrhythmia" es predicted we have a good chance to be correct, but if we predict "Normal" we have a good chance to be wrong! We need to improve sensitivity.





### Second Predictions

Let's works with the second Prediction, that include all the classes and the challenge is to know which of all the arrhythmia classification (or normal) the patient has. Again in the first graph we are looking for the better k (number of Neighbors):

```{r K-Nearest Neighbors Model Second Prediction train, echo=FALSE, warning=FALSE, fig.height = 4}
# Predict region using KNN


knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)

set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 17, 1)),
             trControl = knn.control,
             data = train_set, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")
```


```{r K-Nearest Neighbors Model Second Prediction train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_knn
```

Now the value of k = `r fit_knn$bestTune$k` is  smaller than in the first prediction.
Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_knn)
varimp_knn_p2 <- varImp(fit_knn)$importance
options(digits =7) # default
ggplot(varImp(fit_knn), top = 8) + ggtitle("K-Nearest Neighbors Model Top 8 feature")
```

We noted that the variable are not the same compare with the same model but only looking to predict Normal/Arrhythmia, some of them appear in other importance position and other are new.
This important variables is for each output, that give us more detail that the other measurement that we see in the others mdels used in this analysis.


Now see how well it is the prediction in the validation set:  
```{r K-Nearest Neighbors Model Second Prediction Prediction, echo=FALSE, warning=FALSE, size='scriptsize'}

options(digits =2)

y_hat_knn <- predict(fit_knn, validation_set, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```


```{r K-Nearest Neighbors Model Second Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


As seen above, KNN cannot accurately classify test observations for classes 5 to 8 and between class11 to 16. In total 7 classes with sensitivity = 0 and specificity = 1.
Almost completely misclassifies in class 2 that are observations have been classified to class 1.
A perfect match in class 3 and 9.



For our record, we keep this result:
```{r K-Nearest Neighbors Model Second Prediction Result, echo=FALSE, warning=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "K-Nearest Neighbors",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```









## Decision Tree Model

Decision Tree is the most intuitive solution that we get from data, and it s very popular en health, that the system give you a quick rule to answer your question.

For this model we are going to user the rpart package and the cp (complexity parameter) is our parameter to start moving to get a better accuracy.


### First Prediction
Let's works with the first Prediction and look for the cp (complexity parameter) that increase our accuracy:

```{r Decision Tree Model First Prediction train, echo=FALSE, warning=FALSE, fig.height = 4}
# use cross validation to choose cp

# rpart need Valid Column Names 
train_set2_p1 <- data.frame(train_set_p1)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2_p1, na.action=na.pass)


ggplot(fit_rpart) + ggtitle("Decision Tree")
```


```{r Decision Tree Model First Prediction train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_rpart
```

The cp is `r round(fit_rpart$bestTune,3)` and we get the following decision tree with this:


```{r Decision Tree Model First Prediction plot, echo=FALSE, warning=FALSE}
# access the final model and plot it


rpart.plot(fit_rpart$finalModel, extra = 2, roundint=FALSE,
  box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu", "RdYlGn", "GnYlRd", "BlGnYl","YlGnBl",
    "GyGy", "GyGn")) # specify 13 colors)
```

The variable importance for this method is:

```{r Decision Tree Model First Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_rpart)
varimp_rpart_p1 <- varImp(fit_rpart)$importance
options(digits =7) # default
```

These are predictors are present in the tree too, not all of them because the tree shows less than 20 predictors, and the overall does not show all of them, only 30 are rated, and it is not directly related with the order in the decision tree.

Now let's see how well it is the prediction in the validation set:

```{r Decision Tree Model First Prediction Predict, echo=FALSE, warning=FALSE, size='scriptsize'}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(fit_rpart, data.frame(validation_set_p1), type = "raw")
cm <- confusionMatrix(y_hat_rpart, reference = validation_set_p1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

Let's prune this, using the previous result but applying a new cp value (cp = 0.01) to get a better accuracy and see how it performs. The new decision tree is:

```{r Decision Tree Model First Prediction Prune, echo=FALSE, warning=FALSE}
# prune the tree 
fit_rpart <- rpart(Class.code ~ ., data = train_set2_p1, control = rpart.control(cp = 0.027, minsplit = 2))
pruned_fit <- prune(fit_rpart, cp = 0.01)

plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)
```


```{r Decision Tree Model First Prediction Prune confusionMatrix, echo=FALSE, warning=FALSE, size='scriptsize'}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(pruned_fit, data.frame(validation_set_p1), type = "class")

cm <- confusionMatrix(y_hat_rpart, reference = validation_set_p1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

We can not see any improvement, but the decision tree is similar but not equal under the change made by prune. The decision tree graph is different because the library can not manage the output from prune.

```{r Decision Tree Model First Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_rpart
Actually <- validation_set$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we keep this result:
```{r Decision Tree Model First Prediction Result, echo=FALSE, warning=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "Decision Tree Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary") %>%
  kable_styling(full_width = F)
```



We see improvements in Accuracy and	Sensitivity and a worse value for	Specificity that it is not crucial for this case. The prune applied in this model does not give any improvement in our results.














### Second Predictions

Let's works with the second Prediction with Decision Tree. Again, look for the cp:

```{r Decision Tree Model Second Prediction train, echo=FALSE, warning=FALSE, fig.height = 4}
# use cross validation to choose cp

# rpart need Valid Column Names 
train_set2 <- data.frame(train_set)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2, na.action=na.pass)


ggplot(fit_rpart) + ggtitle("Decision Tree")
```


```{r Decision Tree Model Second Prediction train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_rpart
```

We get a cp = `r round(fit_rpart$bestTune,4)` and the following decision tree was created with this train dataset:


```{r Decision Tree Model Second Prediction plot, echo=FALSE, warning=FALSE}
# access the final model and plot it


rpart.plot(fit_rpart$finalModel, extra = 2, roundint=FALSE,
  box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu", "RdYlGn", "GnYlRd", "BlGnYl","YlGnBl",
    "GyGy", "GyGn")) # specify 13 colors)
```

Again, let's see the predictors which this model consider that help more:

```{r Decision Tree Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_rpart)
varimp_rpart_p2 <- varImp(fit_rpart)$importance
options(digits =7) # default
```

This trained model, now is applied to the validation set. How well does it perform?

```{r Decision Tree Model Second Prediction Predict, echo=FALSE, warning=FALSE, size='scriptsize'}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(fit_rpart, data.frame(validation_set), type = "raw")
cm <- confusionMatrix(y_hat_rpart, reference = validation_set$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

Let's prune this, using the previous result but applying a new cp value (cp = 0.01) to get a better accuracy and see if the prediction improve or not:

```{r Decision Tree Model Second Prediction Prune, echo=FALSE, warning=FALSE}
# prune the tree 
#library(rpart)
fit_rpart <- rpart(Class.code ~ ., data = train_set2, control = rpart.control(cp = 0.033, minsplit = 2))
pruned_fit <- prune(fit_rpart, cp = 0.01)

plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)
```


```{r Decision Tree Model Second Prediction Prune ConfusionMatrix, echo=FALSE, warning=FALSE, size='scriptsize'}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(pruned_fit, data.frame(validation_set), type = "class")

cm <- confusionMatrix(y_hat_rpart, reference = validation_set$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```
We can not see any improvement neither, but the decision tree is similar but not equal under the change made by prune.


```{r Decision Tree Model Second Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_rpart
Actually <- validation_set$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we keep this result:
```{r Decision Tree Model Second Prediction Result, echo=FALSE, warning=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Decision Tree Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


We see a good improvements in Accuracy. The prune applied in this model does not give any improvement in our results.
In 5 classes we can not predict any case (Sensitivity =0 & Specificity=1) and Class 9 we have all the cases right!





## Random Forests Model

We see one decison tree in the previous section. Applying prune we have a different one but with similar accuracy. The idea behind random forest is that we created multiple trees and the class who get more "votes" among all the tress is the predicted value.

In other words, among the tress, witch prediction is the most popular, wins.

Here we have like parameters the value of mtry that we are trying to use to get the better accuracy.


### First Predictions

We start look for mtry to get the better accuracy in this model fro Normal/Arrhythmia prediction:

```{r Random Forests Model First Predictions train, echo=FALSE, warning=FALSE, fig.height = 4}
#Random Forests

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set_p1)



ggplot(fit_rf) + ggtitle("Random Forests")
```


```{r Random Forests Model First Predictions train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_rf
```

the mtry we get is `r fit_rf$bestTune`. The variable importance for this method are:


```{r Random Forests Model First Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_rf)
varimp_rf_p1 <- varImp(fit_rf)$importance
options(digits =7) # default
```

Now see how well it is the prediction in the validation set:

```{r Random Forests Model First Prediction Prediction, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
y_hat_rf <- predict(fit_rf, validation_set_p1, type = "raw")
cm <- confusionMatrix(data = y_hat_rf, reference = validation_set_p1$`Class code`)
cm
options(digits =7) # default
```

```{r Random Forests Model First Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_rf
Actually <- validation_set_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Random Forests Model First Prediction Result, echo=FALSE, warning=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "Random Forest Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary") %>%
  kable_styling(full_width = F)
```



Random Forests get better result over decision tree and previous ones in Accuracy	and Sensitivity. Until now if our best predictor.



### Second Predictions

Let's start with mtry in random forest, now with all the classes:

```{r Random Forests Model Second Predictions train, echo=FALSE, warning=FALSE, fig.height = 4}
#Random Forests

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set)



ggplot(fit_rf) + ggtitle("Random Forests")
```


```{r Random Forests Model Second Predictions train fit, echo=FALSE, warning=FALSE, size='scriptsize'}
fit_rf
```

the mtry we get is `r fit_rf$bestTune`. The variable importance for this method are:


```{r Random Forests Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_rf)
varimp_rf_p2 <- varImp(fit_rf)$importance
#ggplot(varImp(fit_knn))
options(digits =7) # default
```

Now see with the model created using the trining set how well it is the prediction in the validation set:


```{r Random Forests Model Second Prediction Prediction, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
y_hat_rf <- predict(fit_rf, validation_set, type = "raw")
cm <- confusionMatrix(data = y_hat_rf, reference = validation_set$`Class code`) 
cm
options(digits =7) # default
```

```{r Random Forests Model Second Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_rf
Actually <- validation_set$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Random Forests Model Second Prediction Result, echo=FALSE, warning=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Random Forest Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


We see improvements in Accuracy.
In 6 classes we can not predict any case (Sensitivity =0 & Specificity=1) and Class 3 and 9 we have all the cases right!
One aspect to consider is that this method spend more CPU time over the rest.


## Support Vector Machines Model 


The Support Vector Machines (SVM), was created as an objective  a fast and dependable classification algorithm that performs very well with a limited amount of data to analyze. This create a n-dimensional space, one for each predictor, an try to get the space of the output.

A support vector machine allows you to classify data that’s linearly separable, but if it is not linearly separable, you can use the kernel trick to make it work. In this case we let the algorithm to select the kernel and later the parameters cost and gamma.



### First Predictions

For this part we are going to use the caret train method that perform better than svm from e1071 library. The last library was useful to determine that the radial kernel is the best solution. In this model, we are not going to use other parameter than the kernel.

```{r Support Vector Machines Model First Prediction pre-train, include=FALSE}
train_svm <- svm(`Class code` ~ ., data = train_set_p1) # initial approach
train_svm
```


```{r Support Vector Machines Model First Prediction train, echo=FALSE, warning=FALSE, size='scriptsize'}
#Support Vector Classifier:
set.seed(1)
fit_svm <- train(`Class code` ~ ., data = train_set_p1, method="svmRadial")  # caret train method 
fit_svm

y_hat_svm <- predict(fit_svm,validation_set_p1)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm, reference = validation_set_p1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default

```

The values that get the better result are sigma = `r round(fit_svm$bestTune[1],4)` and C = `r fit_svm$bestTune[2]`.
The SVM-Kernel is radial.

The important variables are:

```{r Support Vector Machines Model First Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_svm)
varimp_svm_p1 <- varImp(fit_svm)$importance
options(digits =7) # default
```


The try to get a tune over the SVM but it fail, for that we keep the original value.

```{r Support Vector Machines Model first Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_svm
Actually <- validation_set_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```

For our record, we keep this result:
```{r Support Vector Machines Model First Prediction Result, echo=FALSE, warning=FALSE}

Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "SVM Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary") %>%
  kable_styling(full_width = F)
```


This method did not perform as expected and it is close to KNN and decison tree in result.










### Second Predictions

Again, taking in consideration all the classes, we are going to use radial kernel.

```{r Support Vector Machines Model Second Prediction train, echo=FALSE, warning=FALSE, size='scriptsize'}
#Support Vector Classifier

ctrl_svm <- trainControl(method = "cv", savePred=T)
set.seed(1)
#fit_svm <- svm(`Class code` ~ ., data = train_set) # See note below in .Rmd file
fit_svm <- train(`Class code` ~ ., data = train_set, method="svmRadial", trControl = ctrl_svm)  # caret train method 
fit_svm

y_hat_svm <- predict(fit_svm,validation_set)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm, reference = validation_set$`Class code`) #$overall["Accuracy"]

cm
options(digits =7) # default

```

Note about the code: at the beginning we see that svm from e1071 library works better for multiple output than the caret train method, but a little more investigation shows that with trainControl() we can improve the training with the caret library and then continue using this library as we did with other models.

The SVM-Kernel is radial.

The important variables are:

```{r Support Vector Machines Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE, size='scriptsize'}
options(digits =2)
varImp(fit_svm)
varimp_svm_p2 <- varImp(fit_svm)$importance
options(digits =7) # default
```



```{r Support Vector Machines Model Second Prediction Table, echo=FALSE, size='scriptsize'}
# Table names
Predicted <- y_hat_svm
Actually <- validation_set_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Support Vector Machines Model Second Prediction Result, echo=FALSE, warning=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "SVM Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


This method did not perform as expected with multi-variable too and it is close to KNN and decision tree in result as before.




\newpage