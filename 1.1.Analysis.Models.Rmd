## Evaluation metrics


As we mention previously, we are going to implement models for two purposes:

a. **Detection of cardiac arrhythmia** 
b. **Classification of cardiac arrhythmia**

In our first prediction for detection of cardiac arrhythmia, the data points are classified into two classes: “Normal” & “Arrhythmia”. This model only identifies if the patient is normal (class 1) or suffers from any form of arrhythmia (class 2 to 16). Then the output is a binary variable (is a variable that has two possible outcomes). This will possible if all the instances belonging to classes 2 to 16 were merged to one class. The arrhythmia class will be treated as the ‘Positive’ class.

We are going to see the following evaluation metrics:

* **accuracy** is defined as the overall proportion that is predicted correctly.
* **sensitivity** is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive.
* **specificity** is defined as the ability of an algorithm to not predict a positive when the actual outcome is not a positive.


We can summarize in the following way:  
**High accuracy**: $Y=1⟹\hat{Y}=1$ and $Y=0⟹\hat{Y}=0$  
**High sensitivity**: $Y=1⟹\hat{Y}=1$  
**High specificity**: $Y=0⟹\hat{Y}=0$  


We are looking for accuracy and sensitivity. Why? it is much more important to maximize sensitivity over specificity: failing to predict a arrhythmia  can put the health/life of the patience in risk. It is better than this patience classified as "Arrhythmia" and make all the exams/procedure and when validate that he or she is ok.


In our prediction model for classification of cardiac arrhythmia classified the patient into one of 16 classes, with class 1 representing “Normal” and classes 2 to 16 representing a condition of cardiac arrhythmia. Here we continue with the accuracy concept but now sensitivity and specificity is for class.


Let's show the standard definition in an binary variable:  
**True Positive (TN)** – This is correctly classified as the class if interest/target.  
**True Negative (TN)** – This is correctly classified as not a class of interest/target.  
**False Positive (FP)** – This is wrongly classified as the class of interest/target.  
**False Negative (FN)** – This is wrongly classified as not a class of interest/target.  




```{r Evaluation metrics Binary Table, echo=FALSE}
M <- matrix(c("True Positives (TP)","False Positives (FP)","False Negatives (FN)","True Negatives (TN)"),2,byrow = TRUE) #%>%
rownames(M) <- c("Predicted Positive","Predicted Negative")
colnames(M) <- c("Actually Positive", "Actually Negative")

M %>% knitr::kable("html") %>%
  kable_styling(full_width = F)
```
An these are the definitions:

$$
Accuracy = (TP+TN)/(TP+TN+FP+FN)\\
Sensitivity = TP/(TP+FN)\\
Specificity = TN/(TN+FP)
$$


Whats happened when we have more class? Then we have the same arithmetic for the Accuracy, but we have a value for Sensitivity and Specificity for each class.


```{r Evaluation metrics Multiple Table, echo=FALSE}
M3 <- matrix(c("True A (TA)","False A (FA.B)","False A (FA.C)","False B (FB.A)","True B (TB)","False B (FB.C)","False C (FC.A)","False C (FC.B)","True C (TC)"),3,byrow = TRUE) #%>%
rownames(M3) <- c("Predicted A", "Predicted B", "Predicted C")
colnames(M3) <- c("Actually A", "Actually B", "Actually C")
M3 %>% knitr::kable("html") %>%
  kable_styling(full_width = F)
```

An these are the definitions:

$$
Accuracy = (TA+TB+TC)/(TA+TB+TC+FA.B+FA.C+FB.A+FB.C+FC.A+FC.B)\\
Sensitivity_{A} = TA/(TA+FB.A+FC.A)\\
Specificity_{A} = TA/(TA+FA.B+FA.C)\\
Sensitivity_{B} = TB/(TB+FA.B+FC.B)\\
Specificity_{B} = TB/(TB+FB.A+FB.C)\\
Sensitivity_{C} = TC/(TC+FB.C+FA.C)\\
Specificity_{C} = TC/(TC+FA.C+FB.C)
$$




\newpage



## First Model...just the most common

We are now to create our models. For that we are not to use the validation, we are going to use train dataset.


The most basic and quick approach it is consider that the most common class (mode), in this case it is the "Normal" class, is the best guess. Then the model is:


$$
 \hat{y} = y_{mode} + \varepsilon_{i,u}
$$
$y_{mode}$ the "true" class for all patience and  $\varepsilon_{i,u}$ an independent errors sampled from the same distribution centered at 0.





In the code, for the prediction for the two purposes, we are going to use different dataset to make easier to read the code: the first one, that concern about “Normal” & “Arrhythmia” only, the "Code class" can only has these two values.
In the case of Classification of cardiac arrhythmia, the "Code class" has all the values allowed (13).
Because it is not a big dataset, this duplicity does not affect the computer resources available fo this project.


### First Prediction
Let's works with the first Prediction
```{r Define new dataset for first Prediction, include=FALSE}

# The first "Class code" is "Normal" and all the rest are some kind of "Arrhythmia"
# Train dataset prediction1
train_set1_p1 <- train_set1
train_set1_p1$`Class code` <- ifelse(train_set1_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# validation for prediction1
validation_set1_p1 <- validation_set1
validation_set1_p1$`Class code` <- ifelse(validation_set1_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# Expected value for validation for prediction1
#expected_value_val_p1 <- ifelse(validation_set$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

```


```{r Model just the most common first Prediction, echo=FALSE, warning=FALSE}
predicted_value <- rep("Normal", times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(predicted_value, validation_set1_p1$`Class code`)  # or positive = "Arrhythmia"
#confusionMatrix(predicted_value, expected_value_val_p1)$overall["Accuracy"]
#confusionMatrix(predicted_value, expected_value_val_p1)$byClass["Sensitivity"]
#confusionMatrix(predicted_value, expected_value_val_p1)$byClass["Specificity"]
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]
```


Because an algorithm that calls everything positive ($Y = 1$ no matter what) has perfect specificity, but worse sensitivity. These are completely opposite if we set as less common.
The Accuracy obtained will be are base one.



```{r Model just the most common first Prediction Table, echo=FALSE}

# Table names
Predicted <- predicted_value
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually)
```

For our record, we keep this first model:
```{r ,echo=FALSE}
Pred1_results <- data.frame(Model = "Just the most common",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"] , row.names = NULL)
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)
```

### Second Predictions
Second Predictions, that considered all the classes, the accuracy is the same, but now we have sensitivity and specificity by class:
```{r Model just the most common Second Prediction, echo=FALSE, warning=FALSE}
y_hat <- rep(1, times = nrow(validation_set1)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(y_hat, validation_set1$`Class code`)

#confusionMatrix(y_hat, validation_set$`Class code`)$overall["Accuracy"]
#confusionMatrix(y_hat, validation_set$`Class code`)$byClass[,1:2]
cm$overall["Accuracy"]
cm$byClass[,1:2]
```


```{r Model just the most common Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2)
```


We can see that only class 1 has good sensitivity, as we expected.


For our record, we keep this second model:
```{r Model just the most common Second Prediction Result, echo=FALSE}
Pred2_results <- data.frame(Model = "Just the most common",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL)
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```







## K-Nearest Neighbors Model

Now we continue with the K-Nearest Neighbors.

In base of the concept of distance, this method attempts to find K nearest points of the train data point to the test data point and assigns the class to it on basis of majority for K nearest points. Then the basic question is which value of k give a better solution.

Because we have to few data to train, we use cross-validation in our training process.

### First Prediction

Let's works with the first Prediction  
```{r K-Nearest Neighbors Model first Prediction train, echo=FALSE, warning=FALSE}

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)   


set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 20, 2)),
             data = train_set1_p1, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")

fit_knn
```

We get k = `r fit_knn$bestTune$k`. Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model first Prediction Variable Importance, echo=FALSE, warning=FALSE}
varImp(fit_knn)
ggplot(varImp(fit_knn), top = 20) + ggtitle("K-Nearest Neighbors Model Top 20 feature")
```





With this model we now take the validation dataset to see how well this apply:

```{r K-Nearest Neighbors Model first Prediction Prediction, echo=FALSE, warning=FALSE}
y_hat_knn <- predict(fit_knn, validation_set1_p1, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set1_p1$`Class code`) #$overall["Accuracy"]
cm
```

We see a good improvement in accuracy and sensitivity over the previous mode approach.

```{r K-Nearest Neighbors Model first Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we add this model:
```{r K-Nearest Neighbors Model first Prediction Result,echo=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "K-Nearest Neighbors",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)

```





KNN in overall, has a poor sensitivity and very good specificity. If "Arrhythmia" es predicted we have a good chance to be correct, but if we predict "Normal" we have a good chance to be wrong! We need to improve sensitivity.





### Second Predictions
Let's works with the second Prediction:  
```{r K-Nearest Neighbors Model Second Prediction train, echo=FALSE, warning=FALSE}
# Predict region using KNN

library(caret)

#knn.control <- trainControl(method = "cv", number = 5, #number = 10
#                            classProbs = TRUE,
#                            summaryFunction = twoClassSummary)

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)

set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 17, 1)),
             trControl = knn.control,
             data = train_set1, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")

fit_knn
```

Now the value of k = `r fit_knn$bestTune$k` is slightly bigger than in the first prediction.
Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_knn)
options(digits =7) # default
ggplot(varImp(fit_knn), top = 8) + ggtitle("K-Nearest Neighbors Model Top 8 feature")
```

We noted that the variable are not the same ompare with the same model but only looking to predict Normal/Arrthymia , some of them appear in other importance position and other are new.


Now see how well it is the prediction in the validation set:  
```{r K-Nearest Neighbors Model Second Prediction Prediction, echo=FALSE, warning=FALSE}
#fit knn model

#validation_set_c <- validation_set %>% drop_na("T Vector angles") %>%  select(-P)  %>% drop_na("QRST")  %>% select(-J) %>% drop_na("Heart rate")

#knn_fit <- knn3(`Class code` ~ Sex+Age, data = train_set, k=11)

options(digits =2)

y_hat_knn <- predict(fit_knn, validation_set, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```


```{r K-Nearest Neighbors Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


As seen above, KNN cannot accurately classify test observations for classes 2 and 5 whereas, it completely misclassifies them for class 4. Most of the misclassified observations have been classified to class 1.......



For our record, we keep this result:
```{r K-Nearest Neighbors Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "K-Nearest Neighbors",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```







KNN is a non-parametric method and makes no assumptions. It classifies a test observation to the class that is most common among its neighbors, which in this case is class 1 since the dataset is imbalanced and skewed towards class 1. Hence, a considerable number of patients who suffer from arrhythmia will not be correctly diagnosed if we follow this model. We find the optimal K that achieves the best accuracy is 7, as seen from the graph, by performing repeated crossvalidation. We use 20 different values to try for each parameter here. Overall AUC of KNN is 0.7097 which means the weighted average of efficiency over all the classes to rightly classify observations into their respective classes is only 0.7097.........


### Decision Tree Model

Decision Tree is the most intuitive solution that we get from data, and it s very popular en health, that the system give you a quick rule to answer your question.

for this model we are going to user the rpart package and the cp (complexity parameter) is our parameter to start moving to get a better accuracy.


### First Prediction
Let's works with the first Prediction



### Second Predictions
Let's works with the second Prediction
```{r Decision Tree Model Second Prediction train, echo=FALSE, warning=FALSE}
# use cross validation to choose cp
#library(caret)

# rpart need Valid Column Names 
train_set2 <- data.frame(train_set1)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2, na.action=na.pass)


ggplot(fit_rpart) + ggtitle("Decision Tree")
fit_rpart
```

The cp is `r fit_rpart$bestTune` and we get the following decision tree with this:


```{r Decision Tree Model Second Prediction plot, echo=FALSE, warning=FALSE}
# access the final model and plot it
plot(fit_rpart$finalModel, margin = 0.1)
text(fit_rpart$finalModel, cex = 0.75)
```


```{r Decision Tree Model Second Prediction Predict, echo=FALSE, warning=FALSE}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(fit_rpart, data.frame(validation_set1), type = "raw")
cm <- confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

Let's prune this, using the previous result but applying a new cp value (cp = 0.01) to get a better accuracy:

```{r Decision Tree Model Second Prediction Prune, echo=FALSE, warning=FALSE}
# prune the tree 
#library(rpart)
fit <- rpart(Class.code ~ ., data = train_set2, control = rpart.control(cp = 0.033, minsplit = 2))
pruned_fit <- prune(fit, cp = 0.01)
plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)

# compute accuracy
#y_hat_rpart <- predict(fit, data.frame(validation_set), type = "raw")
options(digits =2)

y_hat_rpart <- predict(pruned_fit, data.frame(validation_set1), type = "class")

cm <- confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```
We can not any improvement, but the decision tree is similar but not equall under the change made by prune.


For our record, we keep this result:
```{r Decision Tree Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Decision Tree Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```



## Random Forests Model

We see one decison tree in the previous section. Applying prune we have a different one but with similar accuracy. The idea behind random forest is that we created multiple trees and the class who get more "votes" among all the tress is the predicted value.

In other words, among the tress, witch prediction is the most popular.

Here we have like parameters the value of mtry that wwe are trying to use to get the better accuracy.

### Second Predictions
```{r Random Forests Model Second Predictions train, echo=FALSE, warning=FALSE}
#Random Forests

#library(randomForest)

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set1)


#fit <- randomForest(`Class code` ~ ., data = trr) 
#plot(fit_rf)
ggplot(fit_rf) + ggtitle("Random Forests")
fit_rf
```

```{r Random Forests Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_rf)
#ggplot(varImp(fit_knn))
options(digits =7) # default
```

```{r Random Forests Model Second Prediction Prediction, echo=FALSE, warning=FALSE}
options(digits =2)
y_hat_rf <- predict(fit_rf, validation_set1, type = "raw")
cm <- confusionMatrix(data = y_hat_rf, reference = validation_set1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```

```{r Random Forests Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_rf
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Random Forests Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Random Forest Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```





## Support Vector Machines Model 


The Support Vector Machines (SVM), was created as an objective  a fast and dependable classification algorithm that performs very well with a limited amount of data to analyze. This create a n-dimensional space, one for each predictor, an try to get the space of the output.

A support vector machine allows you to classify data that’s linearly separable, but if it is not linearly separable, you can use the kernel trick to make it work. In this case we let the algorithm to select the kernel and later the parameters cost and gamma.


### Second Predictions

```{r Support Vector Machines Model Second Prediction train, echo=FALSE, warning=FALSE}
#Support Vector Classifier:
#library(e1071)
set.seed(1)
train_svm <- svm(`Class code` ~ ., data = train_set1)
train_svm

y_hat_svm <- predict(train_svm,validation_set1)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm, reference = validation_set1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default

#Tuning SVM to find the best cost and gamma ..
train_svm_tune <- tune(svm, `Class code` ~ .,data = train_set1, 
              kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))

train_svm_tune

y_hat_svm_tune <- predict(train_svm_tune$best.model,validation_set1)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm_tune, reference = validation_set1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```



For our record, we keep this result:
```{r Support Vector Machines Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "SVM Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```





## no

Some movies are over the average and other under it. Can be see that in the data? Is there some preference for some movies over others? This is the code assuming that there is one effect for the movie itself (one item has better preference than other)

$$
 y_{u,i} = \mu + b_{i} + \varepsilon_{i,u}
$$

 $\mu$ the "true" rating for all movies and $b_{i}$ the average rating for movie i or the "movie effect" and $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0.
 
 
 



We see that adding the movie we get a great improvement in our prediction. But we need to continue because we did not get to the target 





One aspect in the machine learning is to check the data and see id it is leading us to a wrong conclusion. We start with the best and worst movie according to the rating:


The best movies we do not see the ones we expected. Is this an error in our knowledge of movies? Take a look to the column n, which n stand for the number of rated received: it is a value as low as 1 or 2 rated. Let see what is happening with the worse movies:


We see the same behavior.

This lead us to anew model

$$
 y_{u,i} = \mu + b_{i,r} + \varepsilon_{i,u}
$$
 $\mu$ the "true" rating for all movies and $b_{i,r}$ the average rating for movie i or the "movie effect" considering the numbers of votes that the move has and $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0.




```{r Result Regularization Movie_User and Genre Based Model, eval=FALSE}
rmse_results %>% knitr::kable()
rm(new_test_set) 
```


The new RMSE get a little better but enough to reach the target. Then this is our model that we are going to use with validation dataset.




\newpage