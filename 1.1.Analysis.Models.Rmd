## Evaluation metrics


As we mention previously, we are going to implement models for two purposes:

a. Detection of cardiac arrhythmia 
b. Classification of cardiac arrhythmia.

In our first prediction for detection of cardiac arrhythmia, the data points are classified into two classes: “Normal” & “Arrhythmia”. This model only identifies if the patient is normal (class 1) or suffers from any form of arrhythmia (class 2 to 16). Then the output is a binary variable (is a variable that has two possible outcomes). The arrhythmia class will be treated as the ‘Positive’ class.

We are going to see the following evaluation metrics:

* accuracy is defined as the overall proportion that is predicted correctly.
* sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive.
* specificity is defined as the ability of an algorithm to not predict a positive when the actual outcome is not a positive.


We can summarize in the following way:

High accuracy: $Y=1⟹\hat{Y}=1$ and $Y=0⟹\hat{Y}=0$

High sensitivity: $Y=1⟹\hat{Y}=1$

High specificity: $Y=0⟹\hat{Y}=0$


We are looking for accuracy and sensitivity. Why? it is much more important to maximize sensitivity over specificity: failing to predict a arrhythmia ("Positive" class) can put the health/life of the patience in risk. It is better than this patience make all the exams/procedure and when validate that he or she is ok.


In our prediction model for classification of cardiac arrhythmia classified the patient into one of 16 classes, with class 1 representing “Normal” and classes 2 to 16 representing a condition of cardiac arrhythmia. Here we continue with the accuracy concept but now sensitivity is for class.


Let's show the standard definition in an binary variable:\\
True Positive (TN) – This is correctly classified as the class if interest/target.\\
True Negative (TN) – This is correctly classified as not a class of interest/target.\\
False Positive (FP) – This is wrongly classified as the class of interest/target.\\
False Negative (FN) – This is wrongly classified as not a class of interest/target.\\




```{r Evaluation metrics Binary Table, echo=FALSE}
M <- matrix(c("True Positives (TP)","False Positives (FP)","False Negatives (FN)","True Negatives (TN)"),2,byrow = TRUE) #%>%
rownames(M) <- c("Predicted Positive","Predicted Negative")
colnames(M) <- c("Actually Positive", "Actually Negative")
M %>% knitr::kable()
```
An these are the definitions:

$$
Accuracy = (TP+TN)/(TP+TN+FP+FN)\\
Sensitivity = TP/(TP+FN)\\
Specificity = TN/(TN+FP)
$$


Whats happened when we have more class? Then we have the same arithmetic for the Accuracy, but we have a value for Sensitivity and Specificity for each class.


```{r Evaluation metrics Multiple Table, echo=FALSE}
M3 <- matrix(c("True A (TA)","False A (FA.B)","False A (FA.C)","False B (FB.A)","True B (TB)","False B (FB.C)","False C (FC.A)","False C (FC.B)","True C (TC)"),3,byrow = TRUE) #%>%
rownames(M3) <- c("Predicted A", "Predicted B", "Predicted C")
colnames(M3) <- c("Actually A", "Actually B", "Actually C")
M3 %>% knitr::kable()
```

An these are the definitions:

$$
Accuracy = (TA+TB+TC)/(TA+TB+TC+FA.B+FA.C+FB.A+FB.C+FC.A+FC.B)\\
Sensitivity_{A} = TA/(TA+FB.A+FC.A)\\
Specificity_{A} = TA/(TA+FA.B+FA.C)\\
Sensitivity_{B} = TB/(TB+FA.B+FC.B)\\
Specificity_{B} = TB/(TB+FB.A+FB.C)\\
Sensitivity_{C} = TC/(TC+FB.C+FA.C)\\
Specificity_{C} = TC/(TC+FA.C+FB.C)
$$





## First Model...just the most common

We are now to create our models. For that we are not to use the validation, we are going to use train dataset.


The most basic and quick approach it is consider that the most common class (mode), in this case it is the "Normal" class, is the best guess. Then the model is:


$$
 \hat{y} = y_{mode} + \varepsilon_{i,u}
$$
$y_{mode}$ the "true" class for all patience and  $\varepsilon_{i,u}$ an independent errors sampled from the same distribution centered at 0.









Let's works with the first Prediction
```{r Model just the most common first Prediction,echo=FALSE, warning=FALSE}

train_set1_p1$`Class code` <- ifelse(train_set1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

expected_value_1 <- ifelse(validation_set$`Class code` == 1,"Normal","Arrhythmia") %>% factor()
predicted_value <- rep("Normal", times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(predicted_value, expected_value_1)  # or positive = "Arrhythmia"
#confusionMatrix(predicted_value, expected_value_1)$overall["Accuracy"]
#confusionMatrix(predicted_value, expected_value_1)$byClass["Sensitivity"]
#confusionMatrix(predicted_value, expected_value_1)$byClass["Specificity"]
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]
```


```{r}
table(expected_value_1,predicted_value)
```
With the class 2 (arrhythmia) as Positive. Because an algorithm that calls everything negative ($\hat{y} = 0$ no matter what) has perfect specificity, but worse Sensitivity. These are completely opposite if we set as less common. The Accuracy obtained will be are base one.



```{r , echo=FALSE}
#install required packages
#install.packages('gmodels')
#import required library 
library(gmodels)
 
#Computes the crosstable calculations
CrossTable(predicted_value, expected_value_1)
```

For our record, we keep this first model:
```{r ,echo=FALSE}
Pred1_results <- data.frame(Model = "Just the most common",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"] )
Pred1_results %>% knitr::kable()

# Pred1_results <- bind_rows(rmse_results,

```


Second Predictions:
```{r ,warning=FALSE}
y_hat <- rep(1, times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(y_hat, validation_set$`Class code`)

confusionMatrix(y_hat, validation_set$`Class code`)$overall["Accuracy"]
confusionMatrix(y_hat, validation_set$`Class code`)$byClass[,1:2]
cm$overall["Accuracy"]
cm$byClass[,1:2]
```



```{r}
#install required packages
#install.packages('gmodels')
#import required library 
library(gmodels)
 
#Computes the crosstable calculations
CrossTable(y_hat, validation_set$`Class code`)
```


experiment......
```{r, eval = FALSE}
lm_fit <-  lm(`Class code` ~ ., data = train_set1)
  p_hat <- predict(lm_fit, validation_set)
  y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
  y_hat <- predict(lm_fit, validation_set1)
  confusionMatrix(y_hat, validation_set1$`Class code`)$overall["Accuracy"]   #check
```

```{r}
#The lm Function
# fit regression line to predict son's height from father's height
fit <- lm(`Class code` ~ ., data = arrhythmia2)
fit

# summary statistics
#summary(fit)  #check
```









## Model knn

Let's works with the first Prediction
```{r ,warning=FALSE}
train_set1_p1 <- train_set1

train_set1_p1$`Class code` <- ifelse(train_set1$`Class code` == 1,1,2) %>% factor()

expected_value_1 <- ifelse(validation_set$`Class code` == 1,1,2) %>% factor()
#predicted_value <- ifelse(train_set$`Class code` == 1,1,2) %>% factor()

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)#,
                          #  classProbs = TRUE,
                           # summaryFunction = twoClassSummary)

set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 20, 2)),
             data = train_set1_p1, na.action=na.pass)

ggplot(fit_knn)
fit_knn$bestTune$k
fit_knn$finalModel
#  1   2   3   4   5   6   7   8   9  10  14  15  16 
#121  21   7   6   6  12   1   1   4  25   2   2  11 
varImp(fit_knn)



y_hat_knn <- predict(fit_knn, validation_set, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = expected_value_1, positive = "2") #$overall["Accuracy"]
cm
```


```{r ,warning=FALSE}
#delete
cm <- confusionMatrix(predicted_value, expected_value_1, positive = "2")
#confusionMatrix(predicted_value, expected_value_1)$overall["Accuracy"]
#confusionMatrix(predicted_value, expected_value_1)$byClass["Sensitivity"]
#confusionMatrix(predicted_value, expected_value_1)$byClass["Specificity"]
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]
```


Let's works with the second Prediction
```{r}
# Predict region using KNN

library(caret)


#KNN.Control <- trainControl(method = "cv", number =
#                              5,classProbs = TRUE,summaryFunction = twoClassSummary)
#fit <- train(Y ~ .,method = "knn",tuneGrid = expand.grid(k =
#                                                           1:50),trControl = KNN.Control,
#             metric = "ROC",data = df_train.boruta)

knn.control <- trainControl(method = "cv", number = 5, #number = 10
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)

set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 17, 2)),
             data = train_set1, na.action=na.pass)  # set train_set latter, arrhythmia2




                        

fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 17, 1)),
             trControl = knn.control,
             data = train_set1, na.action=na.pass)  # set train_set latter, arrhythmia2
ggplot(fit_knn)
fit_knn$bestTune$k
fit_knn$finalModel
#  1   2   3   4   5   6   7   8   9  10  14  15  16 
#121  21   7   6   6  12   1   1   4  25   2   2  11 
varImp(fit_knn)
```




```{r}
#fit knn model

#validation_set_c <- validation_set %>% drop_na("T Vector angles") %>%  select(-P)  %>% drop_na("QRST")  %>% select(-J) %>% drop_na("Heart rate")

#knn_fit <- knn3(`Class code` ~ Sex+Age, data = train_set, k=11)

y_hat_knn <- predict(fit_knn, validation_set, type = "raw")
confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`) #$overall["Accuracy"]

```

```{r, eval = FALSE}
#no used
#fit knn model


knn_fit <- knn3(`Class code` ~ V261+V228+V113+V148+V202, data = train_set, k=9)

y_hat_knn <- predict(knn_fit, validation_set, type = "class")
confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`)$overall["Accuracy"]

```

```{r}
#fit knn model


knn_fit <- knn3(`Class code` ~ Age+Sex, data = train_set, k=9)

y_hat_knn <- predict(knn_fit, validation_set, type = "class")
confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`)$overall["Accuracy"]

```

aaaa
```{r, eval = FALSE}
    control <- trainControl(method = "cv", number = 10, p = .9)
    train_knn_cv <- train(y ~ ., method = "knn", 
                          data = mnist_27$train,
                          tuneGrid = data.frame(k = seq(9, 71, 2)),
                          trControl = control)
    ggplot(train_knn_cv, highlight = TRUE)
    
    train_knn$results %>% 
      ggplot(aes(x = k, y = Accuracy)) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(x = k, 
                        ymin = Accuracy - AccuracySD,
                        ymax = Accuracy + AccuracySD))
```

### Model rpart


Let's works with the first Prediction




Let's works with the second Prediction
```{r rpart 2nd}
# use cross validation to choose cp
library(caret)
# rpart need Valid Column Names 
train_set2 <- data.frame(train_set1)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2, na.action=na.pass)


ggplot(fit_rpart)

# access the final model and plot it
plot(fit_rpart$finalModel, margin = 0.1)
text(fit_rpart$finalModel, cex = 0.75)
#train_set2 %>% 
#  mutate(y_hat = predict(fit_rpart)) %>% 
#  ggplot() +
#  geom_point(aes(day, margin)) +
#  geom_step(aes(day, y_hat), col="red")



# compute accuracy
y_hat_rpart <- predict(fit_rpart, data.frame(validation_set1), type = "raw")
confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)$overall["Accuracy"]
```

```{r, eval = FALSE}
# prune the tree 
library(rpart)
fit <- rpart(Class.code ~ ., data = train_set2, control = rpart.control(cp = 0.033, minsplit = 2))
pruned_fit <- prune(fit, cp = 0.01)
plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)

# compute accuracy
#y_hat_rpart <- predict(fit, data.frame(validation_set), type = "raw")
y_hat_rpart <- predict(pruned_fit, data.frame(validation_set1))

confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)$overall["Accuracy"]
```




## Model Random Forests

```{r}
#Random Forests

library(randomForest)

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set1)

#fit <- randomForest(`Class code` ~ ., data = trr) 
plot(fit_rf)
```


```{r, eval = FALSE}
library(randomForest)
train_rf <- randomForest(y ~ ., data=mnist_27$train)
confusionMatrix(predict(fit_rf, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]



#  fit <- randomForest(y ~ x, data = dat, nodesize = 50, maxnodes = 25)




# use cross validation to choose parameter
train_rf_2 <- train(y ~ .,
                    method = "Rborist",
                    tuneGrid = data.frame(predFixed = 2, minNode = c(3, 50)),
                    data = mnist_27$train)
confusionMatrix(predict(train_rf_2, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]
```

## Model svm

```{r}
#Support Vector Classifier:
library(e1071)
set.seed(1)
train_svm <- svm(`Class code` ~ .,kernel = "linear",cost = 1,
                     #tuneGrid = data.frame(ranges = seq(0, 2, len = 10)),
                     data = train_set1)

#svc <- svm(Y~.,data = df_train.boruta,kernel = "linear",cost = 1)
tune.out <- tune(svm,`Class code` ~ .,data = train_set1,ranges = list(cost= c(0.001,0.05,0.01,0.1,1,5,10,100)), kernel = "linear")
bestsvc <- tune.out$best.model
summary(bestsvc)
svcpred <- predict(bestsvc,validation_set1)
svcpred <- predict(train_svm,validation_set1)


#y_hat_rpart <- predict(train_rpart, validation_set, type = "raw")

#print(mean(svcpred==validation_set$Y))
#confusionMatrix(svcpred,validation_set$Y)
confusionMatrix(data = svcpred, reference = validation_set$`Class code`)$overall["Accuracy"]

```






## no

Some movies are over the average and other under it. Can be see that in the data? Is there some preference for some movies over others? This is the code assuming that there is one effect for the movie itself (one item has better preference than other)

$$
 y_{u,i} = \mu + b_{i} + \varepsilon_{i,u}
$$

 $\mu$ the "true" rating for all movies and $b_{i}$ the average rating for movie i or the "movie effect" and $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0.
 
 
 



We see that adding the movie we get a great improvement in our prediction. But we need to continue because we did not get to the target 





One aspect in the machine learning is to check the data and see id it is leading us to a wrong conclusion. We start with the best and worst movie according to the rating:


The best movies we do not see the ones we expected. Is this an error in our knowledge of movies? Take a look to the column n, which n stand for the number of rated received: it is a value as low as 1 or 2 rated. Let see what is happening with the worse movies:


We see the same behavior.

This lead us to anew model

$$
 y_{u,i} = \mu + b_{i,r} + \varepsilon_{i,u}
$$
 $\mu$ the "true" rating for all movies and $b_{i,r}$ the average rating for movie i or the "movie effect" considering the numbers of votes that the move has and $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0.




```{r Result Regularization Movie_User and Genre Based Model, eval=FALSE}
rmse_results %>% knitr::kable()
rm(new_test_set) 
```


The new RMSE get a little better but enough to reach the target. Then this is our model that we are going to use with validation dataset.




\newpage