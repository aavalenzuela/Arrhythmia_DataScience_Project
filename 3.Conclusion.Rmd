# Conclusion
*This section that gives a brief summary of the report, its limitations and future work.*

The first thing to note is that we have a big numbers of predictors (large input dimensionality) while the number of observations is really low. This big numbers of predictors has some degree of correlation among them. This is a challenge to the algorithms we use, where random forest look like has some advantages.

Some approach artificially expand the size of the dataset predictors by multiplying/averaging/(other function) some observations. This can work in some cases, required knowledge to be made properly. We are not to apply this here, besides the last point, we already have to many predictors for our algorithm.

For the low number of patients (row in our dataset) we used bootstrapping in our training.

Despite of this low numbers of rows we still divide the dataset in train and validation, because we do not want to brake this rule of independency of the data in the validation set


The initial task of cleaning the data, using this real data, shows its importance, detecting data incomplete, erroneous data, that can lead us to a bad conclusion. More knowledge of physiology can be useful in detect more erroneous data in all the predictors.

In this classifier problem, when we look between “Normal” & “Arrhythmia”, with random forest we get a good result in accuracy and sensitivity:

```{r Conclusion Prediction Normal/Arrhytmia Summary, echo=FALSE}
Pred1_results[4,] %>% data.frame(row.names = NULL) %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F, position = "left")
```

then we can feel we have a good result with a dataset with only `r ncol(arrhythmia)` patients. 

When we work with all the different classification in the arrhythmia, its 15 class, we notices that the distribution of this class are not equal, some of them are not even present in the dataset and the classification of "Others" was no predictable, maybe because under this name several different diseases.
This characteristics of the dataset can not be resolved properly with the model used, and not model can do it.

In the second classifier problem, when we look among all the classification, random forest we get a good result:


```{r Conclusion Prediction All Arrhytmia Classification Summary, echo=FALSE}
Pred2_results[4,] %>% data.frame(row.names = NULL) %>% t() %>%
  knitr::kable(caption = "Prediction All Arrhytmia Classification Summary", "html") %>%
  kable_styling(full_width = F, position = "left")
```

5 point in accuracy compared with the first prediction.

As a summary of the 16 classes (class 1 is normal) we have: 
No data in dataset: 3 
No prediction can be construct with the models used: 6 
Classes can be predicted with our models: 7 


```{r Conclusion Prediction by Classes,echo=FALSE}
cols <- c("red", "yellow", "green")
pie_data_4graph <-results %>% group_by(Prediction) %>% summarise( n=n())
pie(pie_data_4graph$n, labels = pie_data_4graph$Prediction, col = cols,  main = "Prediction by Classes")
```

Can this be improved? Yes, if we have more data that cover our lack of patients with some arrhythmia.
Check if the "Others" can be divided in some way and then look for a rule to detect it.

Other approach that we did not take is trying to create another category in base of group of arrhythmia, for example, all the arrhythmia related with Myocardial, or Synus, because I need medical knowledge that I do not have.


The original problem was that some software can not detect the arrhythmia properly. With this works we can see we can create some level of confidence in the case of “Normal” & “Arrhythmia” and for all the classes but for a more robust solution more data will be necessary to train our model.


