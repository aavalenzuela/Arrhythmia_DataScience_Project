## Evaluation metrics


As we mention previously, we are going to implement models for two purposes:

a. **Detection of cardiac arrhythmia** 
b. **Classification of cardiac arrhythmia**

In our **first** prediction for detection of cardiac arrhythmia, the data points are classified into two classes: “Normal” & “Arrhythmia”. This model only identifies if the patient is normal (class 1) or suffers from any form of arrhythmia (class 2 to 16). Then the output is a binary variable (is a variable that has two possible outcomes). This will possible if all the instances belonging to classes 2 to 16 were merged to one class. The arrhythmia class will be treated as the ‘Positive’ class.

We are going to see the following evaluation metrics:

* **accuracy** is defined as the overall proportion that is predicted correctly.
* **sensitivity** is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive.
* **specificity** is defined as the ability of an algorithm to not predict a positive when the actual outcome is not a positive.


We can summarize in the following way:  
**High accuracy**: $Y=1⟹\hat{Y}=1$ and $Y=0⟹\hat{Y}=0$  
**High sensitivity**: $Y=1⟹\hat{Y}=1$  
**High specificity**: $Y=0⟹\hat{Y}=0$  


We are looking for accuracy and sensitivity. Why? it is much more important to maximize sensitivity over specificity: failing to predict a arrhythmia  can put the health/life of the patience in risk. It is better than this patience classified as "Arrhythmia" and make all the exams/procedure and when validate that he or she is ok.


In our **second** prediction model for classification of cardiac arrhythmia classified the patient into one of 16 classes, with class 1 representing “Normal” and classes 2 to 16 representing a condition of cardiac arrhythmia. Here we continue with the accuracy concept but now sensitivity and specificity is for class.


Let's show the standard definition in an binary variable:  
**True Positive (TN)** – This is correctly classified as the class if interest/target.  
**True Negative (TN)** – This is correctly classified as not a class of interest/target.  
**False Positive (FP)** – This is wrongly classified as the class of interest/target.  
**False Negative (FN)** – This is wrongly classified as not a class of interest/target.  




```{r Evaluation metrics Binary Table, echo=FALSE}
M <- matrix(c("True Positives (TP)","False Positives (FP)","False Negatives (FN)","True Negatives (TN)"),2,byrow = TRUE) #%>%
rownames(M) <- c("Predicted Positive","Predicted Negative")
colnames(M) <- c("Actually Positive", "Actually Negative")

M %>% knitr::kable("html") %>%
  kable_styling(full_width = F)
```
An these are the definitions:

$$
Accuracy = (TP+TN)/(TP+TN+FP+FN)\\
Sensitivity = TP/(TP+FN)\\
Specificity = TN/(TN+FP)
$$


Whats happened when we have more class? Then we have the same arithmetic for the Accuracy, but we have a value for Sensitivity and Specificity for each class.


```{r Evaluation metrics Multiple Table, echo=FALSE}
M3 <- matrix(c("True A (TA)","False A (FA.B)","False A (FA.C)","False B (FB.A)","True B (TB)","False B (FB.C)","False C (FC.A)","False C (FC.B)","True C (TC)"),3,byrow = TRUE) #%>%
rownames(M3) <- c("Predicted A", "Predicted B", "Predicted C")
colnames(M3) <- c("Actually A", "Actually B", "Actually C")
M3 %>% knitr::kable("html") %>%
  kable_styling(full_width = F)
```

An these are the definitions:

$$
Accuracy = (TA+TB+TC)/(TA+TB+TC+FA.B+FA.C+FB.A+FB.C+FC.A+FC.B)\\
Sensitivity_{A} = TA/(TA+FB.A+FC.A)\\
Specificity_{A} = TA/(TA+FA.B+FA.C)\\
Sensitivity_{B} = TB/(TB+FA.B+FC.B)\\
Specificity_{B} = TB/(TB+FB.A+FB.C)\\
Sensitivity_{C} = TC/(TC+FB.C+FA.C)\\
Specificity_{C} = TC/(TC+FA.C+FB.C)
$$




\newpage



## First Model...just the most common

We are now to create our models. For that we are not to use the validation, we are going to use only train dataset.


The most basic and quick approach it is consider that the most common class (mode), in this case it is the "Normal" class, is the best guess. Then the model is:


$$
 \hat{y} = y_{mode} + \varepsilon_{i,u}
$$
$y_{mode}$ the "true" class for all patience and  $\varepsilon_{i,u}$ an independent errors sampled from the same distribution centered at 0.





In the code, for the prediction for the two purposes, we are going to use different dataset to make easier to read the code: the first one, that concern about “Normal” & “Arrhythmia” only, the "Code class" can only has these two values.
In the case of Classification of cardiac arrhythmia, the "Code class" has all the values allowed (13).
Because it is not a big dataset, this duplicity does not affect the computer resources available fo this project.


### First Prediction
Let's works with the first Prediction
```{r Define new dataset for first Prediction, include=FALSE}

# The first "Class code" is "Normal" and all the rest are some kind of "Arrhythmia"
# Train dataset prediction1
train_set1_p1 <- train_set1
train_set1_p1$`Class code` <- ifelse(train_set1_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# validation for prediction1
validation_set1_p1 <- validation_set1
validation_set1_p1$`Class code` <- ifelse(validation_set1_p1$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

# Expected value for validation for prediction1
#expected_value_val_p1 <- ifelse(validation_set$`Class code` == 1,"Normal","Arrhythmia") %>% factor()

```


```{r Model just the most common first Prediction, echo=FALSE, warning=FALSE}
predicted_value <- rep("Normal", times = nrow(validation_set)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(predicted_value, validation_set1_p1$`Class code`)  # or positive = "Arrhythmia"
#confusionMatrix(predicted_value, expected_value_val_p1)$overall["Accuracy"]
#confusionMatrix(predicted_value, expected_value_val_p1)$byClass["Sensitivity"]
#confusionMatrix(predicted_value, expected_value_val_p1)$byClass["Specificity"]
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]
```


Because an algorithm that calls everything positive ($Y = 1$ no matter what) has perfect specificity, but worse sensitivity. These are completely opposite if we set as less common.
The Accuracy obtained will be our base one.



```{r Model just the most common first Prediction Table, echo=FALSE}

# Table names
Predicted <- predicted_value
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually)
```

For our record, we keep this first model:
```{r ,echo=FALSE}
Pred1_results <- data.frame(Model = "Just the most common",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"] , row.names = NULL)
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)
```



### Second Predictions
Second Predictions, that considered all the classes, the accuracy is the same, but now we have sensitivity and specificity by class:
```{r Model just the most common Second Prediction, echo=FALSE, warning=FALSE}
y_hat <- rep(1, times = nrow(validation_set1)) %>% factor() # Using Normal class (1) as the only answer
cm <- confusionMatrix(y_hat, validation_set1$`Class code`)

#confusionMatrix(y_hat, validation_set$`Class code`)$overall["Accuracy"]
#confusionMatrix(y_hat, validation_set$`Class code`)$byClass[,1:2]
cm$overall["Accuracy"]
cm$byClass[,1:2]
```


```{r Model just the most common Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2)
```


We can see that only class 1 has good sensitivity, as we expected.


For our record, we keep this second model:
```{r Model just the most common Second Prediction Result, echo=FALSE}
Pred2_results <- data.frame(Model = "Just the most common",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL)
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


Then this first approach give us a base line to the next ones.




## K-Nearest Neighbors Model

Our first algorithm will be the K-Nearest Neighbors. KNN is a non-parametric method and makes no assumptions.

In base of the concept of distance, this method attempts to find K nearest points of the train data point to the test data point and assigns the class to it on basis of majority for K nearest points. Then the basic question is which value of k give a better solution.

Because we have to few data to train, we use cross-validation in our training process.

### First Prediction

Let's works with the first Prediction  
```{r K-Nearest Neighbors Model first Prediction train, echo=FALSE, warning=FALSE}

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)   


set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(2, 20, 2)),
             data = train_set1_p1, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")

fit_knn
```

We get k = `r fit_knn$bestTune$k`. Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model first Prediction Variable Importance, echo=FALSE, warning=FALSE}
varImp(fit_knn)
ggplot(varImp(fit_knn), top = 20) + ggtitle("K-Nearest Neighbors Model Top 20 feature")
```





With this model we now take the validation dataset to see how well this apply:

```{r K-Nearest Neighbors Model first Prediction Prediction, echo=FALSE, warning=FALSE}
y_hat_knn <- predict(fit_knn, validation_set1_p1, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set1_p1$`Class code`) #$overall["Accuracy"]
cm
```

We see a good improvement in accuracy and sensitivity over the previous mode approach.

```{r K-Nearest Neighbors Model first Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we add this model:
```{r K-Nearest Neighbors Model first Prediction Result,echo=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "K-Nearest Neighbors",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)

```





KNN in overall, has a poor sensitivity and very good specificity. If "Arrhythmia" es predicted we have a good chance to be correct, but if we predict "Normal" we have a good chance to be wrong! We need to improve sensitivity.





### Second Predictions
Let's works with the second Prediction:  
```{r K-Nearest Neighbors Model Second Prediction train, echo=FALSE, warning=FALSE}
# Predict region using KNN

library(caret)

#knn.control <- trainControl(method = "cv", number = 5, #number = 10
#                            classProbs = TRUE,
#                            summaryFunction = twoClassSummary)

knn.control <- trainControl(method = "repeatedcv", number = 20, repeats = 3)

set.seed(1, sample.kind="Rounding")
fit_knn <- train(`Class code` ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 17, 1)),
             trControl = knn.control,
             data = train_set1, na.action=na.pass)

ggplot(fit_knn) + ggtitle("K-Nearest Neighbors")

fit_knn
```

Now the value of k = `r fit_knn$bestTune$k` is slightly bigger than in the first prediction.
Now the variables that this model considers more important are:

```{r K-Nearest Neighbors Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_knn)
options(digits =7) # default
ggplot(varImp(fit_knn), top = 8) + ggtitle("K-Nearest Neighbors Model Top 8 feature")
```

We noted that the variable are not the same compare with the same model but only looking to predict Normal/Arrhythmia, some of them appear in other importance position and other are new.
This important variables is for each output, that give us more detail that the other measurement that we see in the others mdels used in this analysis.


Now see how well it is the prediction in the validation set:  
```{r K-Nearest Neighbors Model Second Prediction Prediction, echo=FALSE, warning=FALSE}
#fit knn model

#validation_set_c <- validation_set %>% drop_na("T Vector angles") %>%  select(-P)  %>% drop_na("QRST")  %>% select(-J) %>% drop_na("Heart rate")

#knn_fit <- knn3(`Class code` ~ Sex+Age, data = train_set, k=11)

options(digits =2)

y_hat_knn <- predict(fit_knn, validation_set, type = "raw")
cm <- confusionMatrix(data = y_hat_knn, reference = validation_set$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```


```{r K-Nearest Neighbors Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_knn
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


As seen above, KNN cannot accurately classify test observations for classes 5 to 8 and between class11 to 16. In total 7 classes with sensitivity = 0 and specificity = 1.
Almost completely misclassifies in class 2 that are observations have been classified to class 1.
A perfect match in class 3 and 9.



For our record, we keep this result:
```{r K-Nearest Neighbors Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "K-Nearest Neighbors",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```







The k of 6 or 7 for these method look a good number and this method perform much better with binary output than the multi-value output.




### Decision Tree Model

Decision Tree is the most intuitive solution that we get from data, and it s very popular en health, that the system give you a quick rule to answer your question.

For this model we are going to user the rpart package and the cp (complexity parameter) is our parameter to start moving to get a better accuracy.


### First Prediction
Let's works with the first Prediction

```{r Decision Tree Model First Prediction train, echo=FALSE, warning=FALSE}
# use cross validation to choose cp
#library(caret)

# rpart need Valid Column Names 
train_set2_p1 <- data.frame(train_set1_p1)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2_p1, na.action=na.pass)


ggplot(fit_rpart) + ggtitle("Decision Tree")
fit_rpart
```

The cp is `r fit_rpart$bestTune` and we get the following decision tree with this:


```{r Decision Tree Model First Prediction plot, echo=FALSE, warning=FALSE}
# access the final model and plot it
plot(fit_rpart$finalModel, margin = 0.1)
text(fit_rpart$finalModel, cex = 0.75)
```

The variable importance for this method is:

```{r Decision Tree Model First Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_rpart)
options(digits =7) # default
```

These arevpredictors are present in the tree too, not all of them because the tree shows less than 20 predictors, and the overall does not show all of them, only 30 are rated, and it is not directly related with the order in the decision tree. Now see how well it is the prediction in the validation set:

```{r Decision Tree Model First Prediction Predict, echo=FALSE, warning=FALSE}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(fit_rpart, data.frame(validation_set1_p1), type = "raw")
cm <- confusionMatrix(y_hat_rpart, reference = validation_set1_p1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

Let's prune this, using the previous result but applying a new cp value (cp = 0.01) to get a better accuracy and see how it performs:

```{r Decision Tree Model First Prediction Prune, echo=FALSE, warning=FALSE}
# prune the tree 
#library(rpart)
fit <- rpart(Class.code ~ ., data = train_set2_p1, control = rpart.control(cp = 0.027, minsplit = 2))
pruned_fit <- prune(fit, cp = 0.01)
plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)

# compute accuracy
#y_hat_rpart <- predict(fit, data.frame(validation_set), type = "raw")
options(digits =2)

y_hat_rpart <- predict(pruned_fit, data.frame(validation_set1_p1), type = "class")

cm <- confusionMatrix(y_hat_rpart, reference = validation_set1_p1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

We can not see any improvement, but the decision tree is similar but not equal under the change made by prune.

```{r Decision Tree Model First Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_rpart
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we keep this result:
```{r Decision Tree Model First Prediction Result, echo=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "Decision Tree Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)
```



We see improvements in Accuracy and	Sensitivity and a worse value for	Specificity that it is not crucial for this case. The prune applied in this model does not give any improvement in our results.














### Second Predictions
Let's works with the second Prediction
```{r Decision Tree Model Second Prediction train, echo=FALSE, warning=FALSE}
# use cross validation to choose cp
#library(caret)

# rpart need Valid Column Names 
train_set2 <- data.frame(train_set1)
set.seed(1, sample.kind="Rounding")
fit_rpart <- train(Class.code ~ ., method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = train_set2, na.action=na.pass)


ggplot(fit_rpart) + ggtitle("Decision Tree")
fit_rpart
```

The cp is `r fit_rpart$bestTune` and we get the following decision tree with this:


```{r Decision Tree Model Second Prediction plot, echo=FALSE, warning=FALSE}
# access the final model and plot it
plot(fit_rpart$finalModel, margin = 0.1)
text(fit_rpart$finalModel, cex = 0.75)
```


```{r Decision Tree Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_rpart)
options(digits =7) # default
```

```{r Decision Tree Model Second Prediction Predict, echo=FALSE, warning=FALSE}
# compute accuracy
options(digits =2)

y_hat_rpart <- predict(fit_rpart, data.frame(validation_set1), type = "raw")
cm <- confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```

Let's prune this, using the previous result but applying a new cp value (cp = 0.01) to get a better accuracy:

```{r Decision Tree Model Second Prediction Prune, echo=FALSE, warning=FALSE}
# prune the tree 
#library(rpart)
fit <- rpart(Class.code ~ ., data = train_set2, control = rpart.control(cp = 0.033, minsplit = 2))
pruned_fit <- prune(fit, cp = 0.01)
plot(pruned_fit, margin = 0.1)
text(pruned_fit, cex = 0.75)

# compute accuracy
#y_hat_rpart <- predict(fit, data.frame(validation_set), type = "raw")
options(digits =2)

y_hat_rpart <- predict(pruned_fit, data.frame(validation_set1), type = "class")

cm <- confusionMatrix(y_hat_rpart, reference = validation_set1$`Class code`)#$overall["Accuracy"]
cm
options(digits =7) # default
```
We can not see any improvement neither, but the decision tree is similar but not equal under the change made by prune.


```{r Decision Tree Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_rpart
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```


For our record, we keep this result:
```{r Decision Tree Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Decision Tree Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


We see a good improvements in Accuracy. The prune applied in this model does not give any improvement in our results.
In 5 classes we can not predict any case (Sensitivity =0 & Specificity=1) and Class 9 we have all the cases right!





## Random Forests Model

We see one decison tree in the previous section. Applying prune we have a different one but with similar accuracy. The idea behind random forest is that we created multiple trees and the class who get more "votes" among all the tress is the predicted value.

In other words, among the tress, witch prediction is the most popular.

Here we have like parameters the value of mtry that we are trying to use to get the better accuracy.


### First Predictions
```{r Random Forests Model First Predictions train, echo=FALSE, warning=FALSE}
#Random Forests

#library(randomForest)

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set1_p1)


#fit <- randomForest(`Class code` ~ ., data = trr) 
#plot(fit_rf)
ggplot(fit_rf) + ggtitle("Random Forests")
fit_rf
```

the mtry we get is `r fit_rf$bestTune`. The variable importance for this method are:


```{r Random Forests Model First Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_rf)
#ggplot(varImp(fit_knn))
options(digits =7) # default
```

Now see how well it is the prediction in the validation set:

```{r Random Forests Model First Prediction Prediction, echo=FALSE, warning=FALSE}
options(digits =2)
y_hat_rf <- predict(fit_rf, validation_set1_p1, type = "raw")
cm <- confusionMatrix(data = y_hat_rf, reference = validation_set1_p1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```

```{r Random Forests Model First Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_rf
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Random Forests Model First Prediction Result, echo=FALSE}
Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "Random Forest Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)
```



Random Forests get better result over decision tree and previous ones.



### Second Predictions
```{r Random Forests Model Second Predictions train, echo=FALSE, warning=FALSE}
#Random Forests

#library(randomForest)

set.seed(1, sample.kind="Rounding")
fit_rf <- train(`Class code` ~ ., method = "rf",
                     tuneGrid = data.frame(mtry = seq(10, 100, len = 25)),
                     data = train_set1)


#fit <- randomForest(`Class code` ~ ., data = trr) 
#plot(fit_rf)
ggplot(fit_rf) + ggtitle("Random Forests")
fit_rf
```

the mtry we get is `r fit_rf$bestTune`. The variable importance for this method are:


```{r Random Forests Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_rf)
#ggplot(varImp(fit_knn))
options(digits =7) # default
```

Now see how well it is the prediction in the validation set:


```{r Random Forests Model Second Prediction Prediction, echo=FALSE, warning=FALSE}
options(digits =2)
y_hat_rf <- predict(fit_rf, validation_set1, type = "raw")
cm <- confusionMatrix(data = y_hat_rf, reference = validation_set1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default
```

```{r Random Forests Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_rf
Actually <- validation_set1$`Class code`
#Computes the crosstable calculations
CrossTableNarrow(Predicted, Actually, digits = 2, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Random Forests Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "Random Forest Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


We see improvements in Accuracy.
In 6 classes we can not predict any case (Sensitivity =0 & Specificity=1) and Class 3 and 9 we have all the cases right!
One aspect to consider is that this method spend more CPU time over the rest.


## Support Vector Machines Model 


The Support Vector Machines (SVM), was created as an objective  a fast and dependable classification algorithm that performs very well with a limited amount of data to analyze. This create a n-dimensional space, one for each predictor, an try to get the space of the output.

A support vector machine allows you to classify data that’s linearly separable, but if it is not linearly separable, you can use the kernel trick to make it work. In this case we let the algorithm to select the kernel and later the parameters cost and gamma.



### First Predictions

For this part we are going to use the caret train method that perform better than svm from e1071 library.

```{r Support Vector Machines Model First Prediction train, echo=FALSE, warning=FALSE}
#Support Vector Classifier:
#library(e1071)
set.seed(1)
#train_svm <- svm(`Class code` ~ ., data = train_set1_p1)
#train_svm

fit_svm <- train(`Class code` ~ ., data = train_set1_p1, method="svmRadial")  # caret train method 
fit_svm

#y_hat_svm <- predict(train_svm,validation_set1_p1)
y_hat_svm <- predict(fit_svm,validation_set1_p1)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm, reference = validation_set1_p1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default

#Tuning SVM to find the best cost and gamma ..
#train_svm_tune <- tune(svm, `Class code` ~ .,data = train_set1_p1, 
#              kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
#train_svm_tune
#train_svm_after_tune <- svm(`Class code` ~ ., data = train_set1_p1, kernel="radial", cost=0.1, gamma=0.5)
#y_hat_svm_tune <- predict(train_svm_after_tune,validation_set1_p1)
```

The values that get the better result are sigma = `r fit_svm$bestTune[1]` and C = `r fit_svm$bestTune[2].
The SVM-Kernel is radial   xxxxxx

The important variables are:

```{r Support Vector Machines Model First Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_svm)
#ggplot(varImp(fit_knn))
options(digits =7) # default
```


The try to get a tune over the SVM but it fail, for that we keep the original value.

```{r Support Vector Machines Model first Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_svm
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```

For our record, we keep this result:
```{r Support Vector Machines Model First Prediction Result, echo=FALSE}

Pred1_results <- bind_rows(Pred1_results,
                           data.frame(Model = "SVM Classifier",
                                     Accuracy = cm$overall["Accuracy"],
                                     Sensitivity = cm$byClass["Sensitivity"],
                                     Specificity = cm$byClass["Specificity"], row.names = NULL ))
Pred1_results %>% knitr::kable(caption = "Prediction Normal/Arrhytmia Summary", "html") %>%
  kable_styling(full_width = F)
```


This method did not perform as expected and it is close to KNN and decison tree in result.










### Second Predictions

The svm from e1071 library works better for multiple output than the caret train method, that why we changed in this part.

```{r Support Vector Machines Model Second Prediction train, echo=FALSE, warning=FALSE}
#Support Vector Classifier:
#library(e1071)
set.seed(1)
train_svm <- svm(`Class code` ~ ., data = train_set1)
train_svm

y_hat_svm <- predict(train_svm,validation_set1)

options(digits =2)
cm <- confusionMatrix(data = y_hat_svm, reference = validation_set1$`Class code`) #$overall["Accuracy"]
cm
options(digits =7) # default


```

The values that get the better result are sigma = `r fit_svm$bestTune[1]` and C = `r fit_svm$bestTune[2].
The SVM-Kernel is radial   xxxxxx

The important variables are:

```{r Support Vector Machines Model Second Prediction Variable Importance, echo=FALSE, warning=FALSE}
options(digits =2)
varImp(fit_svm)
#ggplot(varImp(fit_knn))
options(digits =7) # default
```



```{r Support Vector Machines Model Second Prediction Table, echo=FALSE}
# Table names
Predicted <- y_hat_svm
Actually <- validation_set1_p1$`Class code`
#Computes the crosstable calculations
CrossTable(Predicted, Actually, digits = 3, prop.chisq = FALSE, prop.t=FALSE)
```



For our record, we keep this result:
```{r Support Vector Machines Model Second Prediction Result, echo=FALSE}
Pred2_results <- bind_rows(Pred2_results,
                           data.frame(Model = "SVM Classifier",
                            Accuracy = cm$overall["Accuracy"],
                            Code.1 = row.names(cm$byClass)[1],
                            Sensitivity.1 = cm$byClass[1,1],
                            Specificity.1 = cm$byClass[1,2],
                            Code.2 = row.names(cm$byClass)[2],
                            Sensitivity.2 = cm$byClass[2,1],
                            Specificity.2 = cm$byClass[2,2],
                            Code.3 = row.names(cm$byClass)[3],
                            Sensitivity.3 = cm$byClass[3,1],
                            Specificity.3 = cm$byClass[3,2],
                            Code.4 = row.names(cm$byClass)[4],
                            Sensitivity.4 = cm$byClass[4,1],
                            Specificity.4 = cm$byClass[4,2],
                            Code.5 = row.names(cm$byClass)[5],
                            Sensitivity.5 = cm$byClass[5,1],
                            Specificity.5 = cm$byClass[5,2],
                            Code.6 = row.names(cm$byClass)[6],
                            Sensitivity.6 = cm$byClass[6,1],
                            Specificity.6 = cm$byClass[6,2],
                            Code.7 = row.names(cm$byClass)[7],
                            Sensitivity.7 = cm$byClass[7,1],
                            Specificity.7 = cm$byClass[7,2],
                            Code.8 = row.names(cm$byClass)[8],
                            Sensitivity.8 = cm$byClass[8,1],
                            Specificity.8 = cm$byClass[8,2],
                            Code.9 = row.names(cm$byClass)[9],
                            Sensitivity.9 = cm$byClass[9,1],
                            Specificity.9 = cm$byClass[9,2],
                            Code.10 = row.names(cm$byClass)[10],
                            Sensitivity.10 = cm$byClass[10,1],
                            Specificity.10 = cm$byClass[10,2],
                            Code.11 = row.names(cm$byClass)[11],
                            Sensitivity.11 = cm$byClass[11,1],
                            Specificity.11 = cm$byClass[11,2],
                            Code.12 = row.names(cm$byClass)[12],
                            Sensitivity.12 = cm$byClass[12,1],
                            Specificity.12 = cm$byClass[12,2],
                            Code.13 = row.names(cm$byClass)[13],
                            Sensitivity.13 = cm$byClass[13,1],
                            Specificity.13 = cm$byClass[13,2],
                            row.names = NULL))
Pred2_results %>% t() %>% knitr::kable(caption = "Prediction All Arrhytmia Classification Summary")
```


This method did not perform as expected with multi-variable too and it is close to KNN and decision tree in result as before.




\newpage